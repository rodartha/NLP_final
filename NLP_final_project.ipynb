{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install accelerate\n",
        "!pip install datasets\n",
        "!pip install torch\n",
        "!pip install tqdm\n",
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q3OyZgdXYNdR",
        "outputId": "88756f1f-690e-4296-89b7-3692824532e8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting accelerate\n",
            "  Downloading accelerate-0.25.0-py3-none-any.whl (265 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/265.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m235.5/265.7 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m265.7/265.7 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.1.0+cu118)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.19.4)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.1.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Installing collected packages: accelerate\n",
            "Successfully installed accelerate-0.25.0\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.15.0-py3-none-any.whl (521 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m521.2/521.2 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Collecting pyarrow-hotfix (from datasets)\n",
            "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
            "Collecting dill<0.3.8,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Collecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.18.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.19.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.3)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.18.0->datasets) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.18.0->datasets) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.11.17)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: pyarrow-hotfix, dill, multiprocess, datasets\n",
            "Successfully installed datasets-2.15.0 dill-0.3.7 multiprocess-0.70.15 pyarrow-hotfix-0.6\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.11.17)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3TtR2-t4X99f",
        "outputId": "afa4073d-2796-40bb-c7cb-e071bd3ff67d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "print(torch.cuda.is_available())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ['USER'] = 'rodartha'\n",
        "os.environ['PASS'] = 'REPLACE'\n",
        "os.environ['REPO'] = 'NLP_final'\n",
        "\n",
        "!git clone https://$USER:$PASS@github.com/$USER/$REPO.git\n",
        "\n",
        "%cd NLP_final"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sl6ialFYYilw",
        "outputId": "5ec525c7-36ef-4455-f23a-356eab1bab00"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'NLP_final'...\n",
            "remote: Enumerating objects: 372, done.\u001b[K\n",
            "remote: Counting objects: 100% (258/258), done.\u001b[K\n",
            "remote: Compressing objects: 100% (200/200), done.\u001b[K\n",
            "remote: Total 372 (delta 75), reused 232 (delta 58), pack-reused 114\u001b[K\n",
            "Receiving objects: 100% (372/372), 470.21 MiB | 34.86 MiB/s, done.\n",
            "Resolving deltas: 100% (108/108), done.\n",
            "Updating files: 100% (179/179), done.\n",
            "/content/NLP_final\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wU0gKocJYoHV",
        "outputId": "baf4851b-7a1c-45ee-b30c-40bd57480112"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34manalysis_sets\u001b[0m/                      \u001b[01;34meval_output_anli_r1_large_r2\u001b[0m/      \u001b[01;34meval_output_mega_hans\u001b[0m/\n",
            "\u001b[01;34manli_analysis_r1\u001b[0m/                   \u001b[01;34meval_output_anli_r1_large_r3\u001b[0m/      \u001b[01;34meval_output_mega_r1\u001b[0m/\n",
            "\u001b[01;34manli_analysis_r2\u001b[0m/                   \u001b[01;34meval_output_anli_r1_on_anli\u001b[0m/       \u001b[01;34meval_output_mega_r2\u001b[0m/\n",
            "\u001b[01;34manli_analysis_r3\u001b[0m/                   \u001b[01;34meval_output_anli_r1_r2\u001b[0m/            \u001b[01;34meval_output_mega_r3\u001b[0m/\n",
            "\u001b[01;34manli_only_model\u001b[0m/                    \u001b[01;34meval_output_anli_r1_r3\u001b[0m/            evaluate.py\n",
            "\u001b[01;34manli_r1_model\u001b[0m/                      \u001b[01;34meval_output_anli_whole\u001b[0m/            \u001b[01;34mhans_analysis\u001b[0m/\n",
            "\u001b[01;34manli_r1_model_large\u001b[0m/                \u001b[01;34meval_output_anli_whole_on_r1\u001b[0m/      \u001b[01;34mhans_smaller_model\u001b[0m/\n",
            "\u001b[01;34manli_whole\u001b[0m/                         \u001b[01;34meval_output_anli_whole_on_r2\u001b[0m/      \u001b[01;34mhans_trained_model\u001b[0m/\n",
            "\u001b[01;34mcombined_model\u001b[0m/                     \u001b[01;34meval_output_anli_whole_on_r3\u001b[0m/      \u001b[01;34mhans_training\u001b[0m/\n",
            "\u001b[01;34mcomparisons\u001b[0m/                        \u001b[01;34meval_output_combined\u001b[0m/              helpers.py\n",
            "\u001b[01;34meval_output\u001b[0m/                        \u001b[01;34meval_output_combined_hans\u001b[0m/         \u001b[01;34mhypo_only_analysis\u001b[0m/\n",
            "\u001b[01;34meval_output_anli_only\u001b[0m/              \u001b[01;34meval_output_combined_r1\u001b[0m/           \u001b[01;34mmega_model\u001b[0m/\n",
            "\u001b[01;34meval_output_anli_only_r1\u001b[0m/           \u001b[01;34meval_output_combined_r2\u001b[0m/           README.md\n",
            "\u001b[01;34meval_output_anli_only_r2\u001b[0m/           \u001b[01;34meval_output_combined_r3\u001b[0m/           requirements.txt\n",
            "\u001b[01;34meval_output_anli_only_r3\u001b[0m/           \u001b[01;34meval_output_hans_smaller\u001b[0m/          run.py\n",
            "\u001b[01;34meval_output_anli_r1\u001b[0m/                \u001b[01;34meval_output_hans_smaller_on_hans\u001b[0m/  train_combined.py\n",
            "\u001b[01;34meval_output_anli_r1_large\u001b[0m/          \u001b[01;34meval_output_hans_trained\u001b[0m/          \u001b[01;34mtrained_model\u001b[0m/\n",
            "\u001b[01;34meval_output_anli_r1_large_on_anli\u001b[0m/  \u001b[01;34meval_output_hans_trained_on_hans\u001b[0m/\n",
            "\u001b[01;34meval_output_anli_r1_large_on_hans\u001b[0m/  \u001b[01;34meval_output_mega\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 train_combined.py --per_device_train_batch_size 64 --num_train_epochs 4.0 --output_dir ./mega_model/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "otgoTdwS9MYr",
        "outputId": "a04d4129-0317-4f92-bac3-68096a7affcb"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-11-30 20:52:32.095324: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-30 20:52:32.095380: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-30 20:52:32.095431: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-30 20:52:33.611969: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-small-discriminator and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Preprocessing data... (this takes a little bit, should only happen once per dataset)\n",
            "Map (num_proc=2): 100% 712232/712232 [02:33<00:00, 4652.31 examples/s]\n",
            "  0% 0/44516 [00:00<?, ?it/s]You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "{'loss': 0.7782, 'learning_rate': 4.943840416928745e-05, 'epoch': 0.04}\n",
            "{'loss': 0.6104, 'learning_rate': 4.88768083385749e-05, 'epoch': 0.09}\n",
            "{'loss': 0.5782, 'learning_rate': 4.831521250786235e-05, 'epoch': 0.13}\n",
            "{'loss': 0.555, 'learning_rate': 4.775361667714979e-05, 'epoch': 0.18}\n",
            "{'loss': 0.532, 'learning_rate': 4.719202084643724e-05, 'epoch': 0.22}\n",
            "{'loss': 0.5196, 'learning_rate': 4.663042501572469e-05, 'epoch': 0.27}\n",
            "{'loss': 0.5156, 'learning_rate': 4.606882918501213e-05, 'epoch': 0.31}\n",
            "{'loss': 0.5074, 'learning_rate': 4.550723335429958e-05, 'epoch': 0.36}\n",
            "{'loss': 0.4972, 'learning_rate': 4.4945637523587026e-05, 'epoch': 0.4}\n",
            "{'loss': 0.4849, 'learning_rate': 4.4384041692874475e-05, 'epoch': 0.45}\n",
            "{'loss': 0.4864, 'learning_rate': 4.3822445862161924e-05, 'epoch': 0.49}\n",
            "{'loss': 0.4823, 'learning_rate': 4.3260850031449365e-05, 'epoch': 0.54}\n",
            "{'loss': 0.468, 'learning_rate': 4.2699254200736814e-05, 'epoch': 0.58}\n",
            "{'loss': 0.4618, 'learning_rate': 4.213765837002426e-05, 'epoch': 0.63}\n",
            "{'loss': 0.4656, 'learning_rate': 4.157606253931171e-05, 'epoch': 0.67}\n",
            "{'loss': 0.4544, 'learning_rate': 4.101446670859916e-05, 'epoch': 0.72}\n",
            "{'loss': 0.4642, 'learning_rate': 4.045287087788661e-05, 'epoch': 0.76}\n",
            "{'loss': 0.4446, 'learning_rate': 3.989127504717406e-05, 'epoch': 0.81}\n",
            "{'loss': 0.4495, 'learning_rate': 3.93296792164615e-05, 'epoch': 0.85}\n",
            "{'loss': 0.4488, 'learning_rate': 3.876808338574894e-05, 'epoch': 0.9}\n",
            "{'loss': 0.4437, 'learning_rate': 3.820648755503639e-05, 'epoch': 0.94}\n",
            "{'loss': 0.4346, 'learning_rate': 3.764489172432384e-05, 'epoch': 0.99}\n",
            "{'loss': 0.4034, 'learning_rate': 3.7083295893611286e-05, 'epoch': 1.03}\n",
            "{'loss': 0.4029, 'learning_rate': 3.6521700062898735e-05, 'epoch': 1.08}\n",
            "{'loss': 0.3949, 'learning_rate': 3.5960104232186184e-05, 'epoch': 1.12}\n",
            "{'loss': 0.4, 'learning_rate': 3.539850840147363e-05, 'epoch': 1.17}\n",
            "{'loss': 0.3905, 'learning_rate': 3.4836912570761074e-05, 'epoch': 1.21}\n",
            "{'loss': 0.3977, 'learning_rate': 3.427531674004852e-05, 'epoch': 1.26}\n",
            "{'loss': 0.3991, 'learning_rate': 3.371372090933597e-05, 'epoch': 1.3}\n",
            "{'loss': 0.3984, 'learning_rate': 3.315212507862342e-05, 'epoch': 1.35}\n",
            "{'loss': 0.3916, 'learning_rate': 3.259052924791087e-05, 'epoch': 1.39}\n",
            "{'loss': 0.3937, 'learning_rate': 3.202893341719832e-05, 'epoch': 1.44}\n",
            "{'loss': 0.3936, 'learning_rate': 3.146733758648576e-05, 'epoch': 1.48}\n",
            "{'loss': 0.3884, 'learning_rate': 3.090574175577321e-05, 'epoch': 1.53}\n",
            "{'loss': 0.3903, 'learning_rate': 3.0344145925060653e-05, 'epoch': 1.57}\n",
            "{'loss': 0.3902, 'learning_rate': 2.9782550094348098e-05, 'epoch': 1.62}\n",
            "{'loss': 0.3936, 'learning_rate': 2.9220954263635547e-05, 'epoch': 1.66}\n",
            "{'loss': 0.3845, 'learning_rate': 2.8659358432922995e-05, 'epoch': 1.71}\n",
            "{'loss': 0.3871, 'learning_rate': 2.8097762602210444e-05, 'epoch': 1.75}\n",
            "{'loss': 0.3838, 'learning_rate': 2.753616677149789e-05, 'epoch': 1.8}\n",
            "{'loss': 0.3827, 'learning_rate': 2.6974570940785338e-05, 'epoch': 1.84}\n",
            "{'loss': 0.3767, 'learning_rate': 2.6412975110072786e-05, 'epoch': 1.89}\n",
            "{'loss': 0.3828, 'learning_rate': 2.585137927936023e-05, 'epoch': 1.93}\n",
            "{'loss': 0.3729, 'learning_rate': 2.528978344864768e-05, 'epoch': 1.98}\n",
            "{'loss': 0.362, 'learning_rate': 2.4728187617935125e-05, 'epoch': 2.02}\n",
            "{'loss': 0.3328, 'learning_rate': 2.4166591787222574e-05, 'epoch': 2.07}\n",
            "{'loss': 0.3385, 'learning_rate': 2.360499595651002e-05, 'epoch': 2.11}\n",
            "{'loss': 0.337, 'learning_rate': 2.3043400125797468e-05, 'epoch': 2.16}\n",
            "{'loss': 0.3422, 'learning_rate': 2.2481804295084916e-05, 'epoch': 2.2}\n",
            "{'loss': 0.3298, 'learning_rate': 2.192020846437236e-05, 'epoch': 2.25}\n",
            "{'loss': 0.3317, 'learning_rate': 2.1358612633659807e-05, 'epoch': 2.29}\n",
            "{'loss': 0.3348, 'learning_rate': 2.0797016802947255e-05, 'epoch': 2.34}\n",
            "{'loss': 0.3316, 'learning_rate': 2.0235420972234704e-05, 'epoch': 2.38}\n",
            "{'loss': 0.3349, 'learning_rate': 1.967382514152215e-05, 'epoch': 2.43}\n",
            "{'loss': 0.3385, 'learning_rate': 1.9112229310809598e-05, 'epoch': 2.47}\n",
            "{'loss': 0.3381, 'learning_rate': 1.8550633480097043e-05, 'epoch': 2.52}\n",
            "{'loss': 0.3388, 'learning_rate': 1.7989037649384492e-05, 'epoch': 2.56}\n",
            "{'loss': 0.3371, 'learning_rate': 1.7427441818671937e-05, 'epoch': 2.61}\n",
            "{'loss': 0.3319, 'learning_rate': 1.6865845987959386e-05, 'epoch': 2.65}\n",
            "{'loss': 0.3322, 'learning_rate': 1.6304250157246834e-05, 'epoch': 2.7}\n",
            "{'loss': 0.3401, 'learning_rate': 1.5742654326534283e-05, 'epoch': 2.74}\n",
            "{'loss': 0.3383, 'learning_rate': 1.518105849582173e-05, 'epoch': 2.79}\n",
            "{'loss': 0.3307, 'learning_rate': 1.4619462665109173e-05, 'epoch': 2.83}\n",
            "{'loss': 0.3327, 'learning_rate': 1.4057866834396622e-05, 'epoch': 2.88}\n",
            "{'loss': 0.3327, 'learning_rate': 1.3496271003684069e-05, 'epoch': 2.92}\n",
            "{'loss': 0.3335, 'learning_rate': 1.2934675172971517e-05, 'epoch': 2.97}\n",
            "{'loss': 0.3243, 'learning_rate': 1.2373079342258964e-05, 'epoch': 3.01}\n",
            "{'loss': 0.2925, 'learning_rate': 1.1811483511546411e-05, 'epoch': 3.06}\n",
            "{'loss': 0.2978, 'learning_rate': 1.1249887680833858e-05, 'epoch': 3.1}\n",
            "{'loss': 0.2995, 'learning_rate': 1.0688291850121305e-05, 'epoch': 3.14}\n",
            "{'loss': 0.2951, 'learning_rate': 1.0126696019408752e-05, 'epoch': 3.19}\n",
            "{'loss': 0.298, 'learning_rate': 9.565100188696199e-06, 'epoch': 3.23}\n",
            "{'loss': 0.2958, 'learning_rate': 9.003504357983647e-06, 'epoch': 3.28}\n",
            "{'loss': 0.3001, 'learning_rate': 8.441908527271094e-06, 'epoch': 3.32}\n",
            "{'loss': 0.2985, 'learning_rate': 7.880312696558541e-06, 'epoch': 3.37}\n",
            "{'loss': 0.2984, 'learning_rate': 7.318716865845988e-06, 'epoch': 3.41}\n",
            "{'loss': 0.2982, 'learning_rate': 6.757121035133436e-06, 'epoch': 3.46}\n",
            "{'loss': 0.3002, 'learning_rate': 6.195525204420883e-06, 'epoch': 3.5}\n",
            "{'loss': 0.2996, 'learning_rate': 5.63392937370833e-06, 'epoch': 3.55}\n",
            "{'loss': 0.2959, 'learning_rate': 5.072333542995777e-06, 'epoch': 3.59}\n",
            "{'loss': 0.2946, 'learning_rate': 4.510737712283224e-06, 'epoch': 3.64}\n",
            "{'loss': 0.303, 'learning_rate': 3.949141881570671e-06, 'epoch': 3.68}\n",
            "{'loss': 0.2981, 'learning_rate': 3.387546050858119e-06, 'epoch': 3.73}\n",
            "{'loss': 0.2982, 'learning_rate': 2.825950220145566e-06, 'epoch': 3.77}\n",
            "{'loss': 0.2964, 'learning_rate': 2.264354389433013e-06, 'epoch': 3.82}\n",
            "{'loss': 0.2882, 'learning_rate': 1.7027585587204602e-06, 'epoch': 3.86}\n",
            "{'loss': 0.2978, 'learning_rate': 1.1411627280079073e-06, 'epoch': 3.91}\n",
            "{'loss': 0.2988, 'learning_rate': 5.795668972953545e-07, 'epoch': 3.95}\n",
            "{'loss': 0.2893, 'learning_rate': 1.7971066582801688e-08, 'epoch': 4.0}\n",
            "{'train_runtime': 9541.4595, 'train_samples_per_second': 298.584, 'train_steps_per_second': 4.666, 'train_loss': 0.3813101220981421, 'epoch': 4.0}\n",
            "100% 44516/44516 [2:39:01<00:00,  4.67it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 run.py --do_train --per_device_train_batch_size 32 --num_train_epochs 3.0 --task nli --model trained_model --dataset anli --output_dir ./anli_whole/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fEUSoUSxcG3w",
        "outputId": "8abd9974-2334-4a22-fef1-8d1bf07098d4"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-11-28 18:32:37.598525: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-28 18:32:37.598587: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-28 18:32:37.598622: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-28 18:32:38.779998: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Preprocessing data... (this takes a little bit, should only happen once per dataset)\n",
            "Map (num_proc=2): 100% 162865/162865 [00:56<00:00, 2898.93 examples/s]\n",
            "  0% 0/15270 [00:00<?, ?it/s]You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "{'loss': 0.779, 'learning_rate': 4.8362802881466926e-05, 'epoch': 0.1}\n",
            "{'loss': 0.6979, 'learning_rate': 4.6725605762933864e-05, 'epoch': 0.2}\n",
            "{'loss': 0.6685, 'learning_rate': 4.508840864440079e-05, 'epoch': 0.29}\n",
            "{'loss': 0.6502, 'learning_rate': 4.345121152586772e-05, 'epoch': 0.39}\n",
            "{'loss': 0.6319, 'learning_rate': 4.181401440733464e-05, 'epoch': 0.49}\n",
            "{'loss': 0.6239, 'learning_rate': 4.017681728880157e-05, 'epoch': 0.59}\n",
            "{'loss': 0.6096, 'learning_rate': 3.8539620170268504e-05, 'epoch': 0.69}\n",
            "{'loss': 0.6016, 'learning_rate': 3.690242305173543e-05, 'epoch': 0.79}\n",
            "{'loss': 0.592, 'learning_rate': 3.526522593320236e-05, 'epoch': 0.88}\n",
            "{'loss': 0.5751, 'learning_rate': 3.362802881466929e-05, 'epoch': 0.98}\n",
            "{'loss': 0.5179, 'learning_rate': 3.199083169613621e-05, 'epoch': 1.08}\n",
            "{'loss': 0.4986, 'learning_rate': 3.0353634577603147e-05, 'epoch': 1.18}\n",
            "{'loss': 0.4901, 'learning_rate': 2.871643745907007e-05, 'epoch': 1.28}\n",
            "{'loss': 0.4914, 'learning_rate': 2.7079240340537005e-05, 'epoch': 1.38}\n",
            "{'loss': 0.4997, 'learning_rate': 2.544204322200393e-05, 'epoch': 1.47}\n",
            "{'loss': 0.4874, 'learning_rate': 2.380484610347086e-05, 'epoch': 1.57}\n",
            "{'loss': 0.4868, 'learning_rate': 2.2167648984937787e-05, 'epoch': 1.67}\n",
            "{'loss': 0.4909, 'learning_rate': 2.0530451866404715e-05, 'epoch': 1.77}\n",
            "{'loss': 0.4769, 'learning_rate': 1.8893254747871645e-05, 'epoch': 1.87}\n",
            "{'loss': 0.4696, 'learning_rate': 1.7256057629338573e-05, 'epoch': 1.96}\n",
            "{'loss': 0.4165, 'learning_rate': 1.5618860510805504e-05, 'epoch': 2.06}\n",
            "{'loss': 0.4021, 'learning_rate': 1.3981663392272431e-05, 'epoch': 2.16}\n",
            "{'loss': 0.4021, 'learning_rate': 1.234446627373936e-05, 'epoch': 2.26}\n",
            "{'loss': 0.3895, 'learning_rate': 1.0707269155206287e-05, 'epoch': 2.36}\n",
            "{'loss': 0.3999, 'learning_rate': 9.070072036673216e-06, 'epoch': 2.46}\n",
            "{'loss': 0.3964, 'learning_rate': 7.432874918140144e-06, 'epoch': 2.55}\n",
            "{'loss': 0.3983, 'learning_rate': 5.795677799607073e-06, 'epoch': 2.65}\n",
            "{'loss': 0.3932, 'learning_rate': 4.158480681074002e-06, 'epoch': 2.75}\n",
            "{'loss': 0.3944, 'learning_rate': 2.5212835625409303e-06, 'epoch': 2.85}\n",
            "{'loss': 0.3899, 'learning_rate': 8.840864440078585e-07, 'epoch': 2.95}\n",
            "{'train_runtime': 1780.5728, 'train_samples_per_second': 274.403, 'train_steps_per_second': 8.576, 'train_loss': 0.508458111156571, 'epoch': 3.0}\n",
            "100% 15270/15270 [29:40<00:00,  8.58it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 run.py --do_eval --task nli --dataset anli --model ./hans_smaller_model/ --output_dir ./eval_output_hans_small_r3/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Na-HL8QfIU49",
        "outputId": "c0e7ad50-a2d3-4e7c-b082-ae10ddb5f4b4"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-12-04 19:08:27.755939: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-12-04 19:08:27.755992: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-12-04 19:08:27.756032: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-12-04 19:08:28.927641: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Preprocessing data... (this takes a little bit, should only happen once per dataset)\n",
            "dict_keys(['train_r1', 'dev_r1', 'test_r1', 'train_r2', 'dev_r2', 'test_r2', 'train_r3', 'dev_r3', 'test_r3'])\n",
            "Map (num_proc=2): 100% 1200/1200 [00:00<00:00, 2298.77 examples/s]\n",
            "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "100% 150/150 [00:02<00:00, 54.60it/s]\n",
            "Evaluation results:\n",
            "{'eval_loss': 2.8368704319000244, 'eval_accuracy': 0.3408333361148834, 'eval_runtime': 3.3131, 'eval_samples_per_second': 362.201, 'eval_steps_per_second': 45.275}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git pull origin main"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o8PPyJtqceIT",
        "outputId": "61587ee5-711f-4e8e-d654-55ac7d020f7d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "remote: Enumerating objects: 5, done.\u001b[K\n",
            "remote: Counting objects:  20% (1/5)\u001b[K\rremote: Counting objects:  40% (2/5)\u001b[K\rremote: Counting objects:  60% (3/5)\u001b[K\rremote: Counting objects:  80% (4/5)\u001b[K\rremote: Counting objects: 100% (5/5)\u001b[K\rremote: Counting objects: 100% (5/5), done.\u001b[K\n",
            "remote: Compressing objects: 100% (1/1)\u001b[K\rremote: Compressing objects: 100% (1/1), done.\u001b[K\n",
            "remote: Total 3 (delta 2), reused 3 (delta 2), pack-reused 0\u001b[K\n",
            "Unpacking objects:  33% (1/3)\rUnpacking objects:  66% (2/3)\rUnpacking objects: 100% (3/3)\rUnpacking objects: 100% (3/3), 289 bytes | 289.00 KiB/s, done.\n",
            "From https://github.com/rodartha/NLP_final\n",
            " * branch            main       -> FETCH_HEAD\n",
            "   3d14e27..cbf9225  main       -> origin/main\n",
            "Updating 3d14e27..cbf9225\n",
            "Fast-forward\n",
            " train_combined.py | 4 \u001b[32m++\u001b[m\u001b[31m--\u001b[m\n",
            " 1 file changed, 2 insertions(+), 2 deletions(-)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "need to push anli only model\n",
        "mega model\n",
        "all eval of anli only\n",
        "all eval of mega"
      ],
      "metadata": {
        "id": "ukubShIGe2rZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git add eval_output_hans_small_r1/"
      ],
      "metadata": {
        "id": "J6EfmC9fHv51"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git config --global user.email \"cwpage@umich.edu\"\n",
        "!git config --global user.name \"Rodartha\"\n",
        "!git commit -m \"HANS trained models evaluated on ANLI\"\n",
        "!git push origin main"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vAtgiNT4H4jd",
        "outputId": "44871ee7-14a1-4301-97f5-64240098f1ea"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[main c3433a9] HANS trained models evaluated on ANLI\n",
            " 18 files changed, 6406 insertions(+)\n",
            " create mode 100644 eval_output_hans_small_r1/eval_metrics.json\n",
            " create mode 100644 eval_output_hans_small_r1/eval_predictions.jsonl\n",
            " create mode 100644 eval_output_hans_small_r1/runs/Dec04_19-07-11_82f7af520f71/events.out.tfevents.1701716838.82f7af520f71.1935.0\n",
            " create mode 100644 eval_output_hans_small_r2/eval_metrics.json\n",
            " create mode 100644 eval_output_hans_small_r2/eval_predictions.jsonl\n",
            " create mode 100644 eval_output_hans_small_r2/runs/Dec04_19-07-50_82f7af520f71/events.out.tfevents.1701716876.82f7af520f71.2134.0\n",
            " create mode 100644 eval_output_hans_small_r3/eval_metrics.json\n",
            " create mode 100644 eval_output_hans_small_r3/eval_predictions.jsonl\n",
            " create mode 100644 eval_output_hans_small_r3/runs/Dec04_19-08-29_82f7af520f71/events.out.tfevents.1701716916.82f7af520f71.2322.0\n",
            " create mode 100644 eval_output_hans_trained_r1/eval_metrics.json\n",
            " create mode 100644 eval_output_hans_trained_r1/eval_predictions.jsonl\n",
            " create mode 100644 eval_output_hans_trained_r1/runs/Dec04_19-04-01_82f7af520f71/events.out.tfevents.1701716668.82f7af520f71.1043.0\n",
            " create mode 100644 eval_output_hans_trained_r2/eval_metrics.json\n",
            " create mode 100644 eval_output_hans_trained_r2/eval_predictions.jsonl\n",
            " create mode 100644 eval_output_hans_trained_r2/runs/Dec04_19-05-18_82f7af520f71/events.out.tfevents.1701716725.82f7af520f71.1397.0\n",
            " create mode 100644 eval_output_hans_trained_r3/eval_metrics.json\n",
            " create mode 100644 eval_output_hans_trained_r3/eval_predictions.jsonl\n",
            " create mode 100644 eval_output_hans_trained_r3/runs/Dec04_19-05-59_82f7af520f71/events.out.tfevents.1701716767.82f7af520f71.1603.0\n",
            "Enumerating objects: 39, done.\n",
            "Counting objects: 100% (39/39), done.\n",
            "Delta compression using up to 2 threads\n",
            "Compressing objects: 100% (38/38), done.\n",
            "Writing objects: 100% (38/38), 968.67 KiB | 4.94 MiB/s, done.\n",
            "Total 38 (delta 4), reused 0 (delta 0), pack-reused 0\n",
            "remote: Resolving deltas: 100% (4/4), completed with 1 local object.\u001b[K\n",
            "To https://github.com/rodartha/NLP_final.git\n",
            "   10a27eb..c3433a9  main -> main\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wCZqzH-gnk7v",
        "outputId": "f569289b-4bbc-4cb9-cb0d-0634a05ec8e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    }
  ]
}